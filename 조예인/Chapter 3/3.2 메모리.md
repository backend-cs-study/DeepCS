# 3.2 메모리
- CPU는 그저 '메모리'에 올라와 있는 프로그램의 명령어들을 실행할 뿐임

# 3.2.1 메모리 계층
- 레지스터, 캐시, 메모리, 저장장치

✔️ 메모리 계층 <br>
- 레지스터 : CPU 안에 있 작은 메모리, 휘발성, 속도 가장 빠름, 기억 용량이 가장 적음
- 캐시: L1, L2 캐시를 지칭, 휘발성, 속도 빠름, 기역 용량이 적음. 참고로 L3 캐시도 있음
- 주기억장치 : RAM을 가리킴, 휘발성, 속도 보통, 기역 용량이 보통
- 보조기억장치 : HDD, SSD을 일컬음, 비휘발성, 속도 낮음, 기역 용량 많음
<br><br>
- 램은 하드디스크로부터 일정량의 데이터를 복사해서 임시 저장하고
  - 이를 필요 시마다 CPU에 빠르게 전달하는 역할
- 계층 위로 올라갈수록 가격은 비싸짐
  - 특징 : 용량은 작아지고 속도는 빨라짐
- 이러한 계층이 있는 이유 : 경제성과 캐시 때문
- ex) 16GB RAM은 8만원이면 삼
  - 하지만 16GM SSD는 훨씬 더 싼 가격에 살 수 있음
    - 이러한 경제성 때문에 계층을 두어 관리함
- 이러한 계층 구조는 일상생활에서 경험 가능
  - 계임을 실행하다 보면 '로딩중'이라는 메시지 나오는 것 볼 수 있음
    - 이는 하드디스크 또는 인터넷에서 데이터를 읽어 RAM으로 전송하는 과정이 아직 끝나지 않음을 의미함

## 캐시(cache)
- 데이터를 미리 복사해 놓는 임시 저장소이자
  - 빠른 장치와 느린 장치에서 속도 차이에 따른 병목 현상을 줄이기 위한 메모리
- 이를 통해 데이터를 접근하는 시같이 오래 걸리는 경우를 해결하고
  - 무언가를 다시 계산하는 시간을 절약 가능
- 실제로 메모리와 CPU 사이의 속도 차이가 너무 크기 때문에 그 중간에 레지스터 계층을 둬서 속도 차이를 해결함
  - 캐싱 계층 : 속도 차이를 해결하기 위해 계층과 계층 사이에 있는 계층
    - ex) 캐시 메모리와 보조기억장치 사이에 있는 주기억장치를 보조기억장치의 캐싱 계층이라고 할 수 있음

### 지역성의 원리
- 캐시 계층을 두는 것 말고 캐시를 직접 설정할 때는 어떻게 해야 할까요?
  - 자주 사용하는 데이터를 기반으로 설정해야 함
    - 근거 : 지역성

#### 시간 지역성(temporal locality)
- 최근 사용한 데이터에 다시 접근하려는 특성을 말함
- ex) for 반복문으로 이루어진 코드 안의 변수 i에 계속해서 접근이 이루어짐
- 여기서 데이터는 변수 i이고 최근에 사용했기 때문에 계속 접근해서 +1을 연이어 하는 것을 볼 수 있음

#### 공간 지역성(spatial locality)
- 최근 접근한 데이터를 일고 있는 공간이나 그 가까운 공간에 접근하는 특성
- 공간을 나타내는 배열 arr의 각 요소들에 i가 할당되며
  - 해당 배열에 연속적으로 접근함
  
## 캐시히트와 캐시미스
- 캐시히트 : 캐시에서 원하는 데이터를 찾음
- 캐시미스 : 해당 데이터가 캐시에 없다면 주메모리로 가서 데이터를 찾아오는 것
- 캐시히트를 하게되면 해당 데이터를 제어장치를 거쳐 가져오게 됨
  - 캐시히트의 경우 위치도 가깝고 CPU 내부 버스를 기반으로 작동하기 때문에 빠름
  - 반면에 캐시미스가 발생되면 메모리에서 가져오게 되는데,
    - 이는 시스템 버스를 기반으로 작동하기 때문에 느림

### 캐시매핑
- 캐시가 히트되기 위해 매핑하는 방법
- CPU의 레지스터와 주 메모리(RAM) 간에 데이터를 주고받을 때를 기반으로 설명함
- 레지스터는 주 메모리에 비하면 굉장히 작고 주 메모리는 굉장히 크기 때문에
  - 작은 레지스터가 캐시 계층으로써 역할을 잘 해주려면 이 매핑을 어떻게 하느냐가 중요함

✔️ 캐시매핑 분류 <br>
- 직접 매핑(directed mapping) : 메모리가 1-100이 있고 캐시가 1-10이 있다면 1:1-10, 2:1-20...
  - 이런 식으로 매핑하는 것. 처리가 빠르지만 충돌 발생이 잦음
- 연관 매핑(associative mapping) : 순서를 일치시키지 않고 관련 있는 캐시와 메모리를 매핑함
  - 충돌이 적지만 모든 블록을 탐색해야 해서 속도가 느림
- 집합 연관 매핑(set associative mapping) : 직접 매핑과 연관 매핑을 합쳐 놓은 것
  - 순서는 일치시키지만 집합을 둬서 저장하며 블록화되어 있기 때문에 검색을 좀 더 효율적
  - ex) 메모리가 1-100이 있고 캐시가 1-10이 있다면 캐시 1-5에는 1-50의 데이터를 무작위로 저장시키는 것

### 웹 브라우저의 캐시
- 소프트웨어적인 대표적인 캐시로는 웹 브라우저의 작은 저장소 쿠키, 로컬 스토리지, 세션 스토리지가 있음
- 보통 사용자의 커스텀한 정보나 인증 모듈 관련 사항들을 웹 브라우저에 저장해서
  - 추후 서버에 요청할 때 자신을 나타내는 아이덴티티나 중복 요청 방지를 위해 쓰이며 오리진(origin)에 종속됨

#### 쿠키
- 만료기한이 있는 키-값 저장소
- same site 옵션을 strict로 설정하지 않았을 경우 다른 도메인에서 요청했을 때 자동 전송되며,
  - 4KB까지 데이터를 저장할 수 있고 만료기한을 정할 수 있음
- 쿠키를 설정할 때는 document.cookie로 쿠키를 볼 수 없게 httponly 옵션을 거는 것이 중요하며,
  - 클라이언트 또는 서버에서 만료기한 등을 정할 수 있는데 보통 서버에서 만료기한을 정함

#### 로컬 스토리지
- 만료기한이 없는 키-값 저장소
- 5MB까지 저장할 수 있으며 웹 브라우저를 닫아도 유지됨
- HTML5를 지원하지 않는 웹 브라우저에서는 사용할 수 없음
  - 클라이언트에서만 수정 가능

#### 세션 스토리지
- 만료기한이 없는 키-값 저장소
- 탭 단위로 세션 스토리지를 생성하며, 탭을 닫을 때 해당 데이터가 삭제됨
- 5MB까지 저장할 수 있으며 
  - HTML5를 지원하지 않는 웹 브라우저에서는 사용할 수 없음
    - 클라이언트에서만 수정 가능

### 데이터베이스의 캐싱 계층
- 참고 : 데이터베이스 시스템을 구축할 때도 메인 데이터베이스 위에 레디스(redis) 데이터베이스 계층을 '캐싱 계층'으로 둬서 성능을 향상시키기도 함

---
# 3.2.2 메모리 관리
- 운영체제의 대표적인 할 일 중 하나
- 컴퓨터 내의 한정된 메모리를 극한으로 활용해야 하는 것

## 가상 메모리(virtual memory)
- 메모리 관리 기법의 하나
- 컴퓨터가 실제로 이용 가능한 메모리 자원을 추상화하여
  - 이를 사용하는 사용자들에게 매우 큰 메모리로 보이게 만든느 것을 말함
- 가상 주소(logical address) : 가상적으로 주어진 주소
- 실제 주소(physical address) : 실제 메모리상에 있는 주소
- 가장 주소는 메모리관리장치(MMU)에 의해 실제 주소로 변환되며,
  - 이 덕분에 사용자는 실제 주소를 의식할 필요 없이 프로그램 구축 가능
- 가상 메모리는 가상 주소와 실제 주소가 매핑되어 있고
  - 프로세스의 주소 정보가 들어있는 '페이지 테이블'로 관리됨
  - 이때 속도 향상을 위해 TLB 사용

✔️ 용어 <br>
- TLB : 메모리와 CPU 사이에 있는 주소 변환을 위한 캐시
  - 페이지 테이블에 있는 리스트를 보관하며 
    - CPU가 페이지 테이블까지 가지 않도록 해 속도를 향상시킬 수 있는 캐시 계층

### 스와핑(swapping)
- IF 가상 메모리에는 존재하지만 실제 메모리인 RAM에는 현재 없는 데이터나 코드에 접근할 경우 페이지 폴트 발생
- 이때 메모리에서 당장 사용하지 않는 영역을 하드디스크로 옯기고 
  - 하드디스크의 일부분을 마치 메모리처럼 불러와 쓰는 것을 스와핑이라고 함
    - 이를 통해 마치 페이지 폴트가 일어나지 않은 것처럼 만듦

### 페이지 폴트(page fault)
- 프로세스의 주소 공간에는 존재하지만 지금 이 컴퓨터의 RAM에는 없는 데이터를 접근했을 경우에 발생

✔️ 과정 <br>
1. 어떤 명령어가 유효한 가상 주소에 접근했으나 해당 페이지가 만약 없다면 트랩이 발생되어 운영체제에 알리게 됨
2. 운영체제는 실제 디스크로부터 사용하지 않은 프레임을 찾음
3. 해당 프레임을 실제 메모리에 가져와서 페이지 교체 알고리즘을 기반으로 특정 페이지와 교체함(이때 스와핑이 일어남)
4. 페이지 테이블을 갱신시킨 후 해당 명령어를 다시 시작

✔️ 용어 <br>
- 페이지(page) : 가상 메모리를 사용하는 최소 크기 단위
- 프레임(frame) : 실제 메모리를 사용하는 최소 크기 단위

## 스레싱(thrashing)
- 메모리의 페이지 폴트율이 높은 것
- 컴퓨터의 심각한 성능 저하 초래
- 메모리에 너무 많은 프로세스가 동시에 올라가게 되면 스와핑이 많이 일어나서 발생하는 것
- 페이지 폴트가 일어나면 CPU 이용률이 낮아짐
  - 그렇게 되면 운영체제는 "CPU가 한가한가?"라고 생각하여 가용성을 더 높이기 위해
    - 많은 프로세스를 메모리에 올리게 됨
      - 이와 같은 악순환이 반복되며 스레싱이 일어나게 됨
- 해결 방법 : 메모리를 늘리거나, HDD를 사용한다면 HDD를 SSD로 바꾸는 방법
  - 운영체제에서 해결 방법 : 작업 세트와 PFF

### 작업 세트(working set)
- 프로세스의 과거 사용 이력인 지역성(locality)을 통해 결정된 페이지 집합을 만들어서 미리 메모리에 로드하는 것
- 미리 메모리에 로드하면 탐색에 드는 비용을 줄일 수 있고 스와핑 또한 줄일 수 있음

### PFF(Page Fault Frequency)
- 페이지 폴트 빈도를 조절하는 방법
- 상한선과 하한선을 만드는 방법
- 만약 상한선에 도달한다면 프레임을 늘리고 하한선에 도달한다면 프레임을 줄이는 것

## 메모리 할당
- 메모리에 프로그램을 할당할 때는 시작 메모리 위치, 메모리의 할당 크기를 기반으로 할당함

### 연속 할당
- 메모리에 '연속적으로' 공간을 할당하는 것

#### 고정 분할 방식(fixed partition allocation)
- 메모리를 미리 나누어 관리하는 방식
  - 메모리가 미리 나뉘어 있기 때문에 융통성이 없음
  - 또한 내부 단편화가 발생함

#### 가변 분할 방식(variable partition allocation)
- 매 시점 프로그램의 크기에 맞게 동적으로 메모리를 나눠 사용함
- 내부 단편화는 발생하지 않고 외부 단편화는 발생할 수 있음

✔️ 가변 분할 방식 종류 <br>
- 최초적합 : 위쪽이나 아래쪽부터 시작해서 홀을 찾으면 바로 할당
- 최적적합 : 프로세스의 크기 이상인 공간 중 가장 작은 홀부터 할당
- 최악적합 : 프로세스의 크기와 가장 많이 차이가 나는 홀에 할당

✔️ 용어 <br>
- 내부 단편화(internal fragmentation) 
  - : 메모리를 나눈 크기보다 프로그램이 작아서 들어가지 못하는 공간이 많이 발생하는 현상
- 외부 단편화(external fragmentation)
  - : 메모리를 나눈 크기보다 프로그램이 커서 들어가지 못하는 공간이 많이 발생하는 현상
    - ex) 100MB를 55MB, 45MB로 나눴지만 프로그램의 크기는 70MB일 때 들어가지 못하는 것
- 홀(hole) : 할당할 수 있는 비어 있는 메모리 공간

### 불연속 할당
- 메모리를 연속적으로 할당하지 않는 불연속 할당은
  - 현대 운영체제가 쓰는 방법으로 불연속 할당인 페이징 기법이 있음
    - 메모리를 동일한 크기의 페이지(보통 4KB)로 나누고
    - 프로그램마디 페이지 테이블을 두어 이를 풍해 매모리에 프로그램을 할당하는 것
    - 폐이징 기법 말고도 세그맨테이션, 페이지드 세그멘테이션이 있음

#### 페이징(paging)
- 동일한 크기의 페이지 단위로 나누어 메모리의 서로 다른 위치에 프로세스를 할당함
- 홀의 크기가 균일하지 않은 문제가 없어지지만 주소 변환이 복잡해짐

#### 세그멘테이션(segmentation)
- 페이지 단위가 아닌 의미 단위인 세그먼트로 나누는 방식
- 프로세스를 이루는 메모리는 코드 영역. 데이터 영역, 스택 영역, 힙영 역으로 이루어지는데 
- 코드와 데이터로 나누거나 코드 내의 작은 함수를 세그먼트로 놓고 나눌 수도 있음 
- 장점 : 공유와 보안 측면
- 단점 : 홀 크기가 균일하지 않음

#### 페이지드 세그멘테이션(paged segmentation)
- 프로그램을 의미 단위인 세그먼트로 나눠 공유나 보안 측면에 강점을 두고 
- 임의의 길이가 아닌 동일한 크기의 페이지 단위로 나누는 것

## 페이지 교체 알고리즘
- 메모리는 한정되어 있기 때문에 스와핑이 많이 일어남
- 스외핑은 많이 일어나지 않도록 설계되어야 하며 
  - 이는 페이지 교체 알고리즘을 기반으로 스와핑이 일어남

### 오프라인 알고리즘(offline algorithm)
- 먼 미래에 찹조되는 페이지와 현재 할당하는 페 이저를 바꾸는 알고리즘이며, 가장 좋은 방법
- 그러나 미래에 사용되는 프로세스를 우리가 알 수 없음
- 즉, 사용한 수 없는 알고리즘이지만 가장 좋은 알고리즘이기 때문에 
- 다른 알고리즘과의 성능 비교에 대한 상한기준(upper bound)을 제공함

### FIFO(First In First Out)
- 가장 먼저 온 페이지를 교체 영역에 가장 먼저 놓는 방법

### LRU(Least Recently Used)
- 참조가 가장 오래된 페이지를 바꿈
- 문제점 : '오래된' 것을 파악 하기 위헤 각 페이지마다 계수기, 스택을 두어야 함
-  LRU 구현을 프로 그래밍으로 구현할 때는 보통 두 개의 자료 구조로 구현함
  - 해시 테이블과 이중 연결 리스트
  - 헤시 테이블 : 이중 연결 리스트에서 빠르게 찾을 수 있도록 쓰고, 
  - 이중 연결 리스트는 한정된 메모리를 나타냄

#### NUR(Not Used Recently)
- LRU에서 발전한 NUR 알고리즘이 있음
- 일명 clock 알고리즘이라고 하며 먼저 0과 1을 가진 비트를 둠
  - 1은 최근에 참조되었고 0은 참조되지 않음을 의미함
  - 시계 방향으로 돌면서 0을 찾고 0을 찾은 순간 해당 프로세스를 교체하고, 
    - 해당 부분을 1로 바꾸는 알고리즘

### LFU(Least Frequently Used)
- 가장 참조 횟수가 적은 페이지를 교체
- 즉, 많이 사용되지 않은 것을 교체하는 것

---
## ✅ 기본 질문과 답변

### Q1. 메모리 계층 구조를 설명해보세요. 왜 이런 구조를 사용하는지도 함께 말해주세요.

- **답변:**
- 메모리는 레지스터 → 캐시(L1\~L3) → RAM(주기억장치) → SSD/HDD(보조기억장치) 순으로 구성됩니다.
- 상위 계층일수록 속도는 빠르지만 용량은 작고 가격은 비쌉니다.
- 이러한 계층 구조는 **속도 차이를 줄이고 비용을 절감하기 위한 캐싱 전략**으로 사용됩니다.
- 예를 들어, RAM은 SSD보다 훨씬 빠르지만 비싸므로, 자주 쓰이는 데이터를 RAM에 복사해 두고 필요 시 빠르게 접근합니다.



### Q2. 캐시에서 캐시 히트(Cache Hit)와 캐시 미스(Cache Miss)의 차이를 설명해주세요.

**답변:**

* **캐시 히트**는 CPU가 요청한 데이터가 캐시에 존재해 빠르게 접근할 수 있는 경우를 의미합니다.
* **캐시 미스**는 캐시에 데이터가 없어, 메모리 또는 디스크 등 느린 계층에서 데이터를 다시 로드해야 하는 경우입니다.
  → 백엔드에서는 **자주 조회되는 API 응답을 Redis나 local memory cache에 저장**함으로써 캐시 히트율을 높이고 처리 속도를 개선합니다.



### Q3. 지역성(Locality)의 원리란 무엇이며, 캐시와 어떤 관련이 있나요?

**답변:**
지역성의 원리는 **프로그램이 메모리를 접근하는 방식이 특정 패턴을 따른다**는 개념입니다.

* **시간 지역성**: 최근 사용한 데이터는 다시 사용될 가능성이 높음
* **공간 지역성**: 근처 주소에 접근할 가능성이 높음
  캐시는 이 지역성 원리를 기반으로 데이터를 미리 적재해두어 성능을 향상시킵니다.



### Q4. 캐시 매핑 방식에는 어떤 종류가 있고, 각각 어떤 장단점이 있나요?

**답변:**

1. **직접 매핑 (Direct Mapping)**: 빠르지만 충돌이 많음
2. **완전 연관 매핑 (Fully Associative Mapping)**: 충돌은 적지만 탐색 시간이 김
3. **집합 연관 매핑 (Set Associative Mapping)**: 타협안으로, 일정한 그룹 안에서만 탐색함

백엔드에서 Redis 캐시 키를 hash 기반으로 구성할 때, 충돌 최소화를 위한 고려와도 유사한 개념입니다.



### Q5. 가상 메모리란 무엇인가요? 페이지 테이블과 TLB의 역할도 함께 설명해주세요.

**답변:**
가상 메모리는 **사용자에게 실제보다 더 큰 메모리 공간을 제공**하기 위한 운영체제 기술입니다.

* **가상 주소 ↔ 물리 주소** 매핑은 **페이지 테이블**로 관리되고
* 이때 변환 속도를 높이기 위해 주소 캐시인 \*\*TLB(Translation Lookaside Buffer)\*\*가 사용됩니다.
  이 덕분에 프로세스 간 메모리 충돌 없이 독립된 메모리처럼 동작하게 됩니다.



### Q6. 페이지 폴트(Page Fault)가 발생하는 원인과 처리 과정을 설명해보세요.

**답변:**
페이지 폴트는 **가상 메모리에는 존재하지만, 실제 메모리(RAM)에는 없는 페이지에 접근**할 때 발생합니다.
처리 과정은 다음과 같습니다:

1. 커널에 트랩 발생
2. 디스크에서 필요한 페이지 로딩
3. 페이지 테이블 업데이트
4. 이전 명령어 재실행

이 과정에서 스와핑이 발생하고 속도가 느려지므로, 백엔드 서버는 **메모리 사용 최적화**가 중요합니다.



### Q7. 스레싱(Thrashing)이 무엇인가요? 실무에서는 어떤 문제가 될 수 있을까요?

- **답변:**
- 스레싱은 **지나치게 많은 페이지 폴트로 인해 메모리 접근보다 스와핑에 더 많은 시간이 소모되는 현상**입니다.
- 결과적으로 시스템 성능이 급격히 저하됩니다.
- 백엔드에서는 GC 과도, 캐시 미스 폭주, 대량 API 요청 시 비슷한 증상이 발생할 수 있어, 
- **메모리 제한 설정, GC 튜닝, 서버 수평 확장** 등을 통해 방지합니다.



### Q8. 메모리 할당 방식 중 '연속 할당'과 '불연속 할당'의 차이를 설명해주세요.

**답변:**

* **연속 할당**: 메모리를 연속된 블록으로 할당함. 내부/외부 단편화 문제가 있음
* **불연속 할당**: **페이징(Paging)** 또는 \*\*세그멘테이션(Segmentation)\*\*으로 관리됨

  * 페이징은 고정 크기 단위로, 세그멘테이션은 의미 단위로 나눔
  * 현대 OS는 이 둘을 결합한 **페이지드 세그멘테이션**을 사용



### Q9. 페이지 교체 알고리즘에는 어떤 것들이 있으며, 가장 현실적인 방법은 무엇인가요?

**답변:**

* **FIFO**: 먼저 들어온 페이지 제거
* **LRU**: 가장 오랫동안 사용하지 않은 페이지 제거
* **Clock(NUR)**: 최근 사용되지 않은 페이지 제거 (비트 체크)
* **LFU**: 참조 횟수 가장 적은 페이지 제거

현실적으로는 **LRU나 Clock 알고리즘**이 가장 많이 쓰이며, 백엔드에서도 Redis의 eviction 정책으로 적용됩니다.



### Q10. 브라우저에서 사용하는 캐시(storage) 방식에는 어떤 것이 있으며, 각각 언제 사용되나요?

**답변:**

* **쿠키**: 서버와 자동 전송되는 작은 저장소 (로그인 유지, 인증)
* **로컬 스토리지**: 클라이언트 측에 5MB까지 영구 저장
* **세션 스토리지**: 탭이 닫히면 삭제됨, 일시적 저장에 사용

백엔드에서는 **JWT를 쿠키에 저장하거나, 로컬스토리지를 이용해 토큰을 클라이언트에 보관**하기도 합니다.

---
## ✅ 심화 질문과 답변

### Q1. Redis와 메모리 계층에서의 ‘캐시’는 어떤 관점에서 유사하며, 어떤 점에서 다릅니까?

**답변:**
둘 다 '속도 차이를 줄이기 위한 임시 저장소'라는 점에서 동일한 캐싱 전략을 따릅니다.

* CPU-메모리 간 캐시는 **하드웨어 차원에서 미리 읽어둘 데이터를 예측**해서 빠르게 제공하는 역할
* Redis는 **네트워크 또는 디스크 기반 DB 요청을 줄이기 위해 미리 데이터를 메모리에 저장**해 처리 속도를 높이는 소프트웨어 기반 캐시입니다.

차이는 저장 위치, 휘발성, 구현 대상(하드웨어 vs 소프트웨어)이며, 공통적으로 **시간/공간 지역성**을 기반으로 동작합니다.



### Q2. LRU 알고리즘을 Redis나 Java에서 구현하려면 어떤 자료구조를 선택하겠습니까?

**답변:**
**HashMap + Doubly LinkedList** 조합이 가장 일반적입니다.

* `HashMap`은 O(1) 시간 복잡도로 캐시 항목을 빠르게 찾기 위해
* `Doubly LinkedList`는 사용 순서를 유지하고, 가장 오래된 항목 제거 시 O(1)로 가능하게 하기 위해 사용됩니다.

예를 들어 Spring Boot에서 `@Cacheable`을 직접 구현할 경우에도 이 구조를 따르는 LRUMap 등을 활용할 수 있습니다.



### Q3. 페이지 교체 알고리즘에서 LFU가 LRU보다 적합한 상황은 어떤 경우입니까?

**답변:**
**단기적으로 자주 사용되지 않지만, 장기적으로 꾸준히 접근되는 데이터**가 많을 경우 LFU가 유리합니다.
예:

* 월간 인기 게시글 API
* 백오피스에서 자주 쓰이는 고정 쿼리 결과

반면, LRU는 **최근 사용성**만 따지기 때문에 이런 장기적 데이터는 쉽게 제거될 수 있어 LFU가 더 적합합니다. Redis 4.0 이상에서는 `allkeys-lfu` 정책도 지원됩니다.



### Q4. JVM에서의 힙과 스택 메모리는 OS의 메모리 계층과 어떻게 매핑될 수 있나요?

**답변:**
JVM의 스택 메모리는 일반적으로 \*\*프로세스 스택(유저 공간의 커널 상단에 위치)\*\*에 매핑되며,
힙 메모리는 운영체제가 할당한 **가상 메모리의 힙 영역**에 해당합니다.

OS 입장에서 이 둘은 메모리 보호나 페이지 교체의 대상이 되며, GC나 메모리 누수는 실제 가상 메모리 시스템과 연결되어 **스와핑 유발**로 이어질 수 있습니다.



### Q5. JVM GC와 OS의 스와핑이 동시에 많이 발생할 경우, 어떤 현상이 생기고 어떻게 대응하시겠습니까?

**답변:**

* GC는 힙 정리를 위해 메모리 접근을 많이 하는데, 이때 스와핑이 자주 발생하면 GC가 느려짐 → 전체 시스템 레이턴시 증가
* 반대로 GC가 오래 걸리면 스레드 일시 중지가 길어져 서버 응답이 지연되며, **서버 죽음(zombie JVM)** 현상으로 이어질 수 있음

**대응 방안:**

* 메모리 과사용 원인 파악 (프로파일링 도구 사용)
* GC 튜닝 (G1GC 사용, 힙 사이즈 제한)
* 서버 스케일 아웃
* OS 단에서 **Swappiness 조정**(Linux에서는 `vm.swappiness` 값 줄이기)



### Q6. 페이지드 세그멘테이션이 실제로 백엔드 시스템 설계에 어떤 식으로 적용될 수 있나요?

**답변:**
페이지드 세그멘테이션은 **논리적으로 나눠진 영역을 균일한 단위로 물리 메모리에 나눠 저장**하는 방식입니다.
비슷한 개념은 다음과 같은 곳에 활용됩니다:

* **멀티 테넌시 기반 DB 설계**: 테넌트 단위로 논리 분리, 페이지 단위로 실제 저장소에 할당
* **파일 시스템**: 디렉토리(논리 단위)는 세그먼트, 실제 파일 블록은 페이지처럼 저장
* **Kubernetes 리소스 할당**: CPU/메모리를 파드 단위로 논리 구분하고 cgroup으로 실제 자원 배분

이러한 설계는 보안, 공유 효율성을 동시에 고려해야 하는 시스템에 유리합니다.



### Q7. JVM Heap OutOfMemoryError가 자주 발생할 경우, Redis 캐시를 사용하는 것이 도움이 될까요?

**답변:**
도움이 될 수 있습니다.
JVM 힙에 자주 로딩되는 대용량 데이터를 Redis로 분리하면:

* 힙 크기 절약
* GC 부하 감소
* 빠른 접근 가능 (local cache보다 느리지만 DB보다 빠름)

단, Redis에도 eviction 정책, TTL 설정, 캐시 적중률 분석이 반드시 병행되어야 합니다. 그렇지 않으면 캐시 미스가 더 큰 부하를 유발할 수 있습니다.